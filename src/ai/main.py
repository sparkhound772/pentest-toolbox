"""Script for interacting with OpenAI and Ollama.

Pipe your query to the LLM as a string to the program. The default is to 
forward it as user_message to the OpenAI API and print the response.

Flags:
    --sysm: A string which will be assigned to the system_message variable
            and provides a context or description of the desired behaviour 
            of the LLM. 

    --ollama: Use a locally running LLM in the Ollama runtime via it's REST API.
"""

import sys
import argparse
from openai import OpenAI
from ai import help_strings


def main():
    """Handles the main functionality and prints the reply from the AI.

    Raises:
        SystemExit: Missing piped input.
    """
    parser = argparse.ArgumentParser(description=help_strings.script_description)
    parser.add_argument('--sysm', help=help_strings.sysm_help)
    parser.add_argument('--ollama', action="store_true", help=help_strings.ollama_help)
    args = parser.parse_args()
    
    user_message = None
    if not sys.stdin.isatty():
        user_message = sys.stdin.read().strip() # The piped message to the AI

    try:
        if user_message is None:
            raise SystemExit(1)

    except SystemExit as e:
        if e.code == 1:
            print("Missing piped input")
        else:
            print("Unexpected error")
        sys.exit(e.code)

    if args.sysm:
        system_message = args.sysm # The system/context message to the AI
    else:
        system_message = "You are a helpful assistant."

    if args.ollama:
        client = OpenAI(
            base_url = 'http://localhost:11434/v1',
            api_key='ollama',
        )
        model_name="llama2"
    else:
        client = OpenAI()
        model_name='gpt-3.5-turbo'

    response_object = call_ai(client, system_message, user_message, model_name)

    response_dict = response_object.model_dump()

    response_string = response_dict["choices"][0]["message"]["content"]

    print(response_string)


def call_ai(client, system_message, user_message, model_name):
    """Calls the LLM with provided input and return the response.
    
    Args:
        client: The OpenAI object.
        system_message: Instructs the desired behavour of the LLM.
        user_message: The query from the user.
        model: The LLM model to call.

    Returns:
        An object of type
        <class 'openai.types.chat.chat_completion.ChatCompletion'>
        which carries the response from the LLM along with other
        information that can then be handled by the receiver.
    """
    response = client.chat.completions.create(
        model=model_name,
        messages=[
            {
                "role": "system",
                "content": system_message 
            },
            {
                "role": "user",
                "content": user_message
            }
        ]
    )
    return response


if __name__ == "__main__":
    main()
