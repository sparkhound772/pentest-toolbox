"""Script for interacting with OpenAI and Ollama.

Pipe your query to the LLM as a string to the program. The default is to 
forward it as user_message to the OpenAI API and print the response.

The API also takes system_message which provides a context or description
of the desired behaviour of the LLM. This can be provided to the program
with the --sysm flag in the form of a string or as a file in the directory
system_prompts in the project root directory.

--ollama instead forwards the input to the locally running LLM
in the Ollama runtime via it's REST API.
"""

import sys
import argparse
from openai import OpenAI
from ai import help_strings


def main():
    """Handles the main functionality and prints the reply from the AI.

    Raises:
        SystemExit: Missing piped input.
    """
    parser = argparse.ArgumentParser(description=help_strings.script_description)
    parser.add_argument('--sysm', help=help_strings.sysm_help)
    parser.add_argument('--ollama', type=bool, help=help_strings.ollama_help)
    args = parser.parse_args()

    user_message = None
    if not sys.stdin.isatty():
        user_message = sys.stdin.read().strip() # The piped message to the AI

    try:
        if user_message is None:
            raise SystemExit(1)

    except SystemExit as e:
        if e.code == 1:
            print("Missing piped input")
        else:
            print("Unexpected error")
        sys.exit(e.code)

    system_message = args.sysm # The system/context message to the AI

    client = OpenAI()

    # type(response_object) --> <class 'openai.types.chat.chat_completion.ChatCompletion'>
    response_object = call_ai(client, system_message, user_message)

    response_dict = response_object.model_dump()

    response_string = response_dict["choices"][0]["message"]["content"]

    print(response_string)


def call_ai(client, system_message, user_message):
    """Calls the LLM with provided input and return the response.
    
    Args:
        client: 
        system_message: instructs the desired behavour of the LLM.
        user_message: the query from the user.

    Returns:
        An object of type
        <class 'openai.types.chat.chat_completion.ChatCompletion'>
        which carries the response from the LLM along with other
        information that can then be handled by the receiver.
    """
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {
                "role": "system",
                "content": system_message 
            },
            {
                "role": "user",
                "content": user_message
            }
        ]
    )
    return response


if __name__ == "__main__":
    main()
