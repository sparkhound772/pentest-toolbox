import argparse
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import sys

def is_valid_url(base_url, url):
    """Check if a URL belongs to the same website as the base URL."""
    return urlparse(url).netloc == urlparse(base_url).netloc

def scrape_site(start_url, output=None):
    """Scrape the site for local URLs."""
    base_url = f"{urlparse(start_url).scheme}://{urlparse(start_url).netloc}"
    visited_urls = set()
    urls_to_visit = [start_url]

    # Only open the file once if output is specified.
    file = open(output, 'a') if output else None

    try:
        while urls_to_visit:
            current_url = urls_to_visit.pop(0)
            if current_url in visited_urls:
                continue

            visited_urls.add(current_url)
            try:
                response = requests.get(current_url)
                if response.headers.get('Content-Type', '').startswith('text/html'):
                    soup = BeautifulSoup(response.text, 'html.parser')
                    for link in soup.find_all('a', href=True):
                        href = link['href']
                        absolute_url = urljoin(current_url, href)
                        if is_valid_url(base_url, absolute_url) and absolute_url not in visited_urls:
                            if file:
                                file.write(f"{absolute_url}\n")
                            else:
                                print(absolute_url)
                            urls_to_visit.append(absolute_url)

                if output:
                    # Dynamically update the count of found URLs on the same line if output file is specified
                    sys.stdout.write(f"\rFound URLs: {len(visited_urls)}" + " " * 20)
                    sys.stdout.flush()

            except requests.RequestException as e:
                sys.stderr.write(f"\nRequest failed: {e}\n")
                sys.stderr.flush()

    finally:
        if file:
            file.close()
            print(f"\nURLs have been written to {output}")
        elif output:
            # Final count is printed on a new line after the loop completes if output file is specified.
            print(f"\rTotal Found URLs: {len(visited_urls)}".ljust(50))

def main():
    """Main function to parse arguments and control the script flow."""
    parser = argparse.ArgumentParser(description="Scrape a website for local URLs.")
    parser.add_argument("start_url", help="The starting URL to begin scraping from.")
    parser.add_argument("-o", "--output", help="Optional: Output file to write the URLs to continuously.", default=None)

    args = parser.parse_args()

    scrape_site(args.start_url, args.output)

if __name__ == "__main__":
    main()
